{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "contact: Mark.schutera@kit.edu\n",
    "\n",
    "\n",
    "# Deep Reinforcement Learning with Deep-Q-Network (DQN)\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to implement a Deep-Q-Network that is able to do a classic control. The approaches are build upon the paper by DeepMind: Playing Atari with Deep Reinforcement Learning [paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), which first introduces the notion of a Deep Q-Network.\n",
    "<img src=\"graphics/atari_play.png\" width=\"700\"><br>\n",
    "<center> Fig. 1: Breakout environment of the Atari game </center>\n",
    "\n",
    "## Core idea\n",
    "As you probably remember from the lecture, during trial and error we can learn a policy for our Atari game, and model it within our Q-matrix. This is done with a deep neural network. After training, this Q-matrix gives us an estimate of the expected reward when taking action a in state s: Q(s, a).\n",
    "Playing the action with the maximum Q-value in any given state is the same as playing optimal, or following a full exploitation strategy.\n",
    "\n",
    "## OpenAI Gym\n",
    "[OpenAI Gym](https://gym.openai.com/docs/) is a library that can simulate a large number of reinforcement learning environments, including Atari games (these need to be installed additionaly). You will need Python 3.5+\n",
    "\n",
    ">pip install gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking our cart pole on a first ride\n",
    "Now that you have gym installed you can load the 'Pendulum-v0' environment of Atari.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym module\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adisaba/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [-0.62096734  0.78383644  0.62204748] reward:  -4.886862995935415\n",
      "observation:  [-0.66524989  0.74662078  1.15704492] reward:  -5.059880174146855\n",
      "observation:  [-0.73501691  0.67804878  1.95726317] reward:  -5.420116607278223\n",
      "observation:  [-0.81175464  0.58399863  2.42917556] reward:  -6.126302008045042\n",
      "observation:  [-0.88941889  0.45709303  2.97843764] reward:  -6.930695297295492\n",
      "observation:  [-0.9566193   0.29134089  3.58191627] reward:  -8.002316935557456\n",
      "observation:  [-0.99458071  0.10396738  3.82945276] reward:  -9.382563356839903\n",
      "observation:  [-0.99537117 -0.09610528  4.00818874] reward:  -10.692945963182936\n",
      "observation:  [-0.96146246 -0.27493624  3.64539081] reward:  -10.884402508936926\n",
      "observation:  [-0.89593945 -0.44417621  3.63462388] reward:  -9.527750619403111\n",
      "observation:  [-0.81749807 -0.57593134  3.06976739] reward:  -8.513009452209062\n",
      "observation:  [-0.73113727 -0.68223038  2.74131841] reward:  -7.3328474206375\n",
      "observation:  [-0.65043032 -0.75956593  2.23673253] reward:  -6.467332009679708\n",
      "observation:  [-0.58604083 -0.81028153  1.63973695] reward:  -5.693930573715516\n",
      "observation:  [-0.53536894 -0.84461831  1.22438961] reward:  -5.09715295638315\n",
      "observation:  [-0.5174231  -0.85572971  0.42215314] reward:  -4.712567860307673\n",
      "observation:  [-0.51858893 -0.8550237  -0.02725895] reward:  -4.491139773063169\n",
      "observation:  [-0.55873817 -0.82934411 -0.95327488] reward:  -4.481117519019842\n",
      "observation:  [-0.62215936 -0.78289063 -1.57268666] reward:  -4.772297617029099\n",
      "observation:  [-0.71290874 -0.70125682 -2.44279064] reward:  -5.278775513945019\n",
      "observation:  [-0.81525957 -0.57909571 -3.19079752] reward:  -6.18946114875733\n",
      "observation:  [-0.90597603 -0.42332898 -3.6100479 ] reward:  -7.388571750350397\n",
      "observation:  [-0.97333308 -0.22939642 -4.11318261] reward:  -8.618966652631647\n",
      "observation:  [-0.99993206 -0.01165665 -4.3960121 ] reward:  -10.161259065829944\n",
      "observation:  [-0.97572607  0.21899461 -4.64881738] reward:  -11.731637365874235\n",
      "observation:  [-0.89877147  0.43841742 -4.6610628 ] reward:  -10.69365817794127\n",
      "observation:  [-0.78724966  0.61663439 -4.2124654 ] reward:  -9.397218292281991\n",
      "observation:  [-0.66921878  0.74306543 -3.4635805 ] reward:  -7.91431664536677\n",
      "observation:  [-0.54452031  0.83874766 -3.14679694] reward:  -6.510410646271778\n",
      "observation:  [-0.43931816  0.89833154 -2.4195513 ] reward:  -5.59861072004412\n",
      "observation:  [-0.35586578  0.93453708 -1.819984  ] reward:  -4.688868028029627\n",
      "observation:  [-0.30548426  0.95219713 -1.06786731] reward:  -4.074169725097268\n",
      "observation:  [-0.29539101  0.95537644 -0.21164389] reward:  -3.654007496925819\n",
      "observation:  [-0.33183841  0.94333625  0.76773953] reward:  -3.5069229365179373\n",
      "observation:  [-0.41390882  0.91031835  1.76984105] reward:  -3.7072642097624082\n",
      "observation:  [-0.53307156  0.84607016  2.70966033] reward:  -4.306337625583472\n",
      "observation:  [-0.6722531   0.7403214   3.50042386] reward:  -5.285098151147223\n",
      "observation:  [-0.80467902  0.59371009  3.95773276] reward:  -6.5527916118650005\n",
      "observation:  [-0.91053672  0.4134282   4.18891588] reward:  -7.848092158843975\n",
      "observation:  [-0.98126273  0.19267448  4.64657832] reward:  -9.128940736597734\n",
      "observation:  [-0.99859575 -0.05297674  4.93777046] reward:  -10.848994094590184\n",
      "observation:  [-0.95650351 -0.29172082  4.86047739] reward:  -11.977615420650173\n",
      "observation:  [-0.86757142 -0.49731261  4.48945993] reward:  -10.46070684395402\n",
      "observation:  [-0.74262276 -0.66970996  4.26639494] reward:  -8.886658979331202\n",
      "observation:  [-0.61375495 -0.78949659  3.52340361] reward:  -7.620165719785383\n",
      "observation:  [-0.48482969 -0.87460858  3.09279115] reward:  -6.222654450384916\n",
      "observation:  [-0.38727946 -0.92196238  2.16978971] reward:  -5.273487763685802\n",
      "observation:  [-0.31828614 -0.94799469  1.47515758] reward:  -4.345694060399725\n",
      "observation:  [-0.2729725  -0.96202184  0.94879054] reward:  -3.809077985092817\n",
      "observation:  [-0.26804686 -0.96340588  0.10232813] reward:  -3.5031496902360173\n",
      "observation:  [-0.29297918 -0.95611882 -0.51952273] reward:  -3.3950566487547893\n",
      "observation:  [-0.35988755 -0.93299569 -1.41612192] reward:  -3.5183606156806695\n",
      "observation:  [-0.45469003 -0.89064975 -2.07753587] reward:  -3.9601080546920753\n",
      "observation:  [-0.56753296 -0.82335068 -2.629645  ] reward:  -4.605327832277851\n",
      "observation:  [-0.69190246 -0.72199099 -3.21228692] reward:  -5.419149692179417\n",
      "observation:  [-0.80879728 -0.58808754 -3.55966686] reward:  -6.485391146393032\n",
      "observation:  [-0.91173268 -0.41078403 -4.10756799] reward:  -7.582298612143875\n",
      "observation:  [-0.97755309 -0.21068926 -4.22067676] reward:  -9.077940851427238\n",
      "observation:  [-0.99997461 -0.00712614 -4.10307603] reward:  -10.36566009944971\n",
      "observation:  [-0.98236837  0.1869556  -3.90376756] reward:  -11.510264724475302\n",
      "observation:  [-0.92620929  0.37700976 -3.97007021] reward:  -10.249177927613117\n",
      "observation:  [-0.83478386  0.5505778  -3.92981082] reward:  -9.168932868421706\n",
      "observation:  [-0.72193524  0.69196063 -3.6229018 ] reward:  -8.090949525689878\n",
      "observation:  [-0.60451531  0.79659353 -3.14875471] reward:  -6.964620572354481\n",
      "observation:  [-0.48798425  0.87285243 -2.78756784] reward:  -5.922140383354509\n",
      "observation:  [-0.39175795  0.92006832 -2.14474777] reward:  -5.105852825929892\n",
      "observation:  [-0.3147519   0.94917398 -1.64692468] reward:  -4.355698723788157\n",
      "observation:  [-0.26742578  0.96357846 -0.98949481] reward:  -3.847217016798242\n",
      "observation:  [-0.24746654  0.96889644 -0.41311841] reward:  -3.4900456402146394\n",
      "observation:  [-0.26177238  0.96512964  0.29587145] reward:  -3.3326150652026776\n",
      "observation:  [-0.30566627  0.95213872  0.91559886] reward:  -3.3788632232436826\n",
      "observation:  [-0.38911012  0.92119125  1.78054555] reward:  -3.6246389243186843\n",
      "observation:  [-0.50814011  0.86127442  2.66717128] reward:  -4.201456322232041\n",
      "observation:  [-0.63570215  0.77193444  3.11787651] reward:  -5.13913436278782\n",
      "observation:  [-0.76510022  0.64391121  3.64558371] reward:  -6.078539922046063\n",
      "observation:  [-0.88513599  0.46533245  4.31178407] reward:  -7.29385201538181\n",
      "observation:  [-0.96744822  0.25306905  4.56317814] reward:  -8.922316296527686\n",
      "observation:  [-0.9998428   0.01773059  4.76239484] reward:  -10.409766821692033\n",
      "observation:  [-0.97342726 -0.22899642  4.97556194] reward:  -12.028324325655529\n",
      "observation:  [-0.89584378 -0.44436913  4.58846682] reward:  -10.94896119183869\n",
      "observation:  [-0.78454116 -0.62007674  4.16741183] reward:  -9.294164315982828\n",
      "observation:  [-0.65707084 -0.75382883  3.70058748] reward:  -7.851235412750971\n",
      "observation:  [-0.53469546 -0.84504483  3.05558222] reward:  -6.603392061686408\n",
      "observation:  [-0.42470962 -0.90532963  2.51012426] reward:  -5.491989403604303\n",
      "observation:  [-0.33794346 -0.94116641  1.87820338] reward:  -4.668009703313176\n",
      "observation:  [-0.29491164 -0.95552453  0.90735799] reward:  -4.025129937202542\n",
      "observation:  [-0.28778324 -0.95769557  0.14903402] reward:  -3.5799031385260207\n",
      "observation:  [-0.31233676 -0.94997145 -0.51481031] reward:  -3.472032639695948\n",
      "observation:  [-0.36282265 -0.93185821 -1.0728663 ] reward:  -3.5937993532556867\n",
      "observation:  [-0.44872736 -0.89366871 -1.88091177] reward:  -3.8873531256259874\n",
      "observation:  [-0.56433466 -0.82554612 -2.68572664] reward:  -4.500441923554731\n",
      "observation:  [-0.70244435 -0.71173867 -3.58397882] reward:  -5.43551248389932\n",
      "observation:  [-0.83139317 -0.55568462 -4.05568172] reward:  -6.805387065725037\n",
      "observation:  [-0.93121629 -0.36446703 -4.32251617] reward:  -8.160633152004353\n",
      "observation:  [-0.98960897 -0.14378488 -4.57550831] reward:  -9.533203877713012\n",
      "observation:  [-0.99525556  0.09729529 -4.8346887 ] reward:  -11.078398879431557\n",
      "observation:  [-0.9448865   0.32739807 -4.72198215] reward:  -11.604299475278424\n",
      "observation:  [-0.84086044  0.541252   -4.76753573] reward:  -10.118589410541832\n",
      "observation:  [-0.71035613  0.70384243 -4.17733832] reward:  -8.877638695920876\n",
      "observation:  [-0.57498373  0.81816484 -3.5483961 ] reward:  -7.318847947007711\n",
      "observation:  [-0.44918804  0.89343724 -2.93455875] reward:  -6.0262622716827705\n",
      "observation:  [-0.34644573  0.93807002 -2.24153608] reward:  -5.009140911659967\n",
      "observation:  [-0.28595771  0.95824224 -1.2754759 ] reward:  -4.209503258516725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [-0.26524265  0.96418169 -0.43100281] reward:  -3.6259714077025986\n",
      "observation:  [-0.28897995  0.95733515  0.49411151] reward:  -3.4032370422393132\n",
      "observation:  [-0.35081796  0.93644368  1.30566552] reward:  -3.499141102025509\n",
      "observation:  [-0.44227128  0.89688133  1.9937017 ] reward:  -3.8924552388918605\n",
      "observation:  [-0.55107788  0.83445382  2.51051775] reward:  -4.515104183761019\n",
      "observation:  [-0.68352222  0.72992971  3.37843631] reward:  -5.274536734213726\n",
      "observation:  [-0.81055328  0.5856649   3.85037977] reward:  -6.539701206560693\n",
      "observation:  [-0.91714938  0.39854362  4.31543514] reward:  -7.812287289358149\n",
      "observation:  [-0.98325253  0.18224834  4.53311507] reward:  -9.324581102212454\n",
      "observation:  [-0.99879682 -0.04903992  4.64664408] reward:  -10.806594554032536\n",
      "observation:  [-0.95909107 -0.28309771  4.75925631] reward:  -11.723882736122459\n",
      "observation:  [-0.87542163 -0.48336009  4.34932999] reward:  -10.415358739768077\n",
      "observation:  [-0.76866528 -0.63965123  3.7911106 ] reward:  -8.847685419411546\n",
      "observation:  [-0.64969182 -0.7601977   3.39145835] reward:  -7.428029266167583\n",
      "observation:  [-0.54696674 -0.83715434  2.56884679] reward:  -6.342203200317574\n",
      "observation:  [-0.45699162 -0.889471    2.08253174] reward:  -5.281279821559065\n",
      "observation:  [-0.40379777 -0.91484827  1.17891436] reward:  -4.619867226673836\n",
      "observation:  [-0.37411912 -0.92738066  0.64435166] reward:  -4.0860309078863235\n",
      "observation:  [-0.38825591 -0.9215516  -0.30583078] reward:  -3.8634668713899862\n",
      "observation:  [-0.4268365  -0.90432881 -0.84506851] reward:  -3.88944565285524\n",
      "observation:  [-0.50275203 -0.86443068 -1.71575532] reward:  -4.12035108237077\n",
      "observation:  [-0.59385596 -0.80457138 -2.18127012] reward:  -4.695691139484294\n",
      "observation:  [-0.69243728 -0.72147807 -2.58037607] reward:  -5.346906404171627\n",
      "observation:  [-0.80274365 -0.59632427 -3.34040447] reward:  -6.123262900116363\n",
      "observation:  [-0.9065164  -0.4221706  -4.06151781] reward:  -7.38256257765\n",
      "observation:  [-0.9778471  -0.20932046 -4.49916918] reward:  -8.971349559139101\n",
      "observation:  [-0.99962786  0.02727896 -4.76324638] reward:  -10.613838812213798\n",
      "observation:  [-0.96546776  0.26052254 -4.72562105] reward:  -11.967793399851828\n",
      "observation:  [-0.87720347  0.4801188  -4.74453484] reward:  -10.518243027861116\n",
      "observation:  [-0.75292405  0.65810742 -4.35024432] reward:  -9.224950988227782\n",
      "observation:  [-0.61699948  0.78696356 -3.75139544] reward:  -7.765290941806305\n",
      "observation:  [-0.47981936  0.8773673  -3.28950774] reward:  -6.406475036395195\n",
      "observation:  [-0.34827549  0.93739222 -2.89436054] reward:  -5.375213873146838\n",
      "observation:  [-0.23520239  0.97194642 -2.3660799 ] reward:  -4.550596441753333\n",
      "observation:  [-0.16893216  0.98562768 -1.35361274] reward:  -3.8330765350521694\n",
      "observation:  [-0.13639376  0.9906547  -0.65851842] reward:  -3.2128014074945086\n",
      "observation:  [-0.14564824  0.98933644  0.18695875] reward:  -2.959785778841114\n",
      "observation:  [-0.19602655  0.98059859  1.02272069] reward:  -2.9518531415352633\n",
      "observation:  [-0.28165965  0.95951438  1.76438293] reward:  -3.23077695990206\n",
      "observation:  [-0.40078461  0.91617231  2.53699483] reward:  -3.757352187395941\n",
      "observation:  [-0.54956926  0.83544816  3.38950951] reward:  -4.57781087481881\n",
      "observation:  [-0.7120688   0.70210969  4.2118362 ] reward:  -5.784460369610353\n",
      "observation:  [-0.86263052  0.50583454  4.96013641] reward:  -7.361028971727579\n",
      "observation:  [-0.96256568  0.27104854  5.11733885] reward:  -9.281081887529673\n",
      "observation:  [-0.99980727  0.01963195  5.09697945] reward:  -10.84126111143526\n",
      "observation:  [-0.96929552 -0.24589875  5.36160041] reward:  -12.347326177395379\n",
      "observation:  [-0.86968007 -0.49361582  5.35591414] reward:  -11.246388669645501\n",
      "observation:  [-0.71920879 -0.69479401  5.03781872] reward:  -9.761165708222501\n",
      "observation:  [-0.55633853 -0.83095574  4.25379885] reward:  -8.174342753194356\n",
      "observation:  [-0.40846912 -0.91277214  3.38393366] reward:  -6.481108068643171\n",
      "observation:  [-0.26792144 -0.96344076  2.99082613] reward:  -5.11523784456856\n",
      "observation:  [-0.14328603 -0.98968132  2.54908065] reward:  -4.291088562232471\n",
      "observation:  [-0.05184926 -0.99865492  1.83816779] reward:  -3.589600005642927\n",
      "observation:  [ 0.00898103 -0.99995967  1.21707343] reward:  -2.9716672844262937\n",
      "observation:  [ 0.03967819 -0.99921251  0.61414929] reward:  -2.5883544236902214\n",
      "observation:  [ 0.01967729 -0.99980638 -0.40020094] reward:  -2.3851284872960097\n",
      "observation:  [-0.05217621 -0.99863789 -1.43756956] reward:  -2.425656355782882\n",
      "observation:  [-0.15991303 -0.98713111 -2.16805278] reward:  -2.8407926240379138\n",
      "observation:  [-0.2966357  -0.95499071 -2.81130606] reward:  -3.4682063612692726\n",
      "observation:  [-0.45235613 -0.89183739 -3.36475199] reward:  -4.295771984096539\n",
      "observation:  [-0.6228236  -0.78236229 -4.05882323] reward:  -5.294607116304513\n",
      "observation:  [-0.78580431 -0.61847521 -4.6329783 ] reward:  -6.679101939752568\n",
      "observation:  [-0.91061604 -0.41325346 -4.81553607] reward:  -8.274560378242619\n",
      "observation:  [-0.9858282  -0.16775805 -5.14937888] reward:  -9.69327731479723\n",
      "observation:  [-0.99655755  0.08290383 -5.03108285] reward:  -11.4932114206992\n",
      "observation:  [-0.9501394   0.31182547 -4.68229202] reward:  -11.889824999944404\n",
      "observation:  [-0.85947357  0.51118018 -4.38887358] reward:  -10.170224847124953\n",
      "observation:  [-0.73293022  0.68030382 -4.23239429] reward:  -8.714718207790003\n",
      "observation:  [-0.58452242  0.81137756 -3.96655822] reward:  -7.522408755618757\n",
      "observation:  [-0.44334385  0.89635162 -3.29931142] reward:  -6.391921430218624\n",
      "observation:  [-0.31394582  0.9494409  -2.79959245] reward:  -5.211264437367893\n",
      "observation:  [-0.22394853  0.97460097 -1.86964206] reward:  -4.35851996145106\n",
      "observation:  [-0.16554762  0.9862018  -1.19101529] reward:  -3.577666258665804\n",
      "observation:  [-0.13430366  0.99094022 -0.63205095] reward:  -3.1608524905797855\n",
      "observation:  [-0.13934415  0.99024401  0.10176706] reward:  -2.9487070357361347\n",
      "observation:  [-0.18551062  0.98264226  0.9358481 ] reward:  -2.9275435383867716\n",
      "observation:  [-0.2537433   0.96727159  1.39913573] reward:  -3.179322317467966\n",
      "observation:  [-0.35983825  0.9330147   2.23092532] reward:  -3.5354488751780617\n",
      "observation:  [-0.4973296   0.86756168  3.04846935] reward:  -4.257617097016857\n",
      "observation:  [-0.64890927  0.76086579  3.71264057] reward:  -5.302920233678003\n",
      "observation:  [-0.79586811  0.60547003  4.28579964] reward:  -6.562854964108104\n",
      "observation:  [-0.9214627   0.38846684  5.02778288] reward:  -8.04674966599944\n",
      "observation:  [-0.99230077  0.12385144  5.49593801] reward:  -10.05124391241901\n",
      "observation:  [-0.9884756  -0.15138025  5.52269492] reward:  -12.125565637719603\n",
      "observation:  [-0.90760758 -0.41981957  5.62564091] reward:  -11.989975444249195\n",
      "observation:  [-0.76975899 -0.63833463  5.18172612] reward:  -10.500662543032343\n",
      "observation:  [-0.5962996  -0.80276198  4.79159845] reward:  -8.684251358331274\n",
      "observation:  [-0.41207044 -0.91115199  4.28316705] reward:  -7.17901658327967\n",
      "observation:  [-0.23790256 -0.97128902  3.69038775] reward:  -5.817022975482889\n",
      "observation:  [-0.10028192 -0.99495906  2.79510151] reward:  -4.6428618628596885\n",
      "observation:  [ 0.00257767 -0.99999668  2.06056881] reward:  -3.574332163336295\n",
      "observation:  [ 0.05623178 -0.99841774  1.07367567] reward:  -2.88639832984281\n",
      "observation:  [ 0.07922209 -0.99685699  0.46087482] reward:  -2.4099160125494854\n",
      "observation:  [ 0.0792989  -0.99685088  0.00154104] reward:  -2.249480662322205\n",
      "observation:  [ 0.03302888 -0.9994544  -0.92694721] reward:  -2.2257698199057288\n",
      "observation:  [-0.03750366 -0.99929649 -1.4109469 ] reward:  -2.453768400628999\n",
      "observation:  [-0.14435809 -0.98952551 -2.14703565] reward:  -2.7857422162419505\n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "# Reset, it returns the starting frame\n",
    "frame = env.reset()\n",
    "\n",
    "for _ in range(100000):\n",
    "    # Perform a random action, returns the new frame, reward and whether the game is over\n",
    "    \n",
    "    '''\n",
    "    Implement to sample a random action from the action space within the loaded environment\n",
    "    action = environment.action_space.sample()\n",
    "    '''\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, is_done, info = env.step(action)\n",
    "    print('observation: ', observation, 'reward: ', reward)\n",
    "    if is_done: break\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks nice, yet the actions are random and thus it is time to better understand our environment. And to implement our Deep-Q-Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "The observation is made up of cos(theta), sin(theta) and theta dot. \n",
    "Theta is normalized between -pi and pi.\n",
    "\n",
    "## Action\n",
    "Joint effort -2.0 to +2.0\n",
    "Write a function to discretize the continuous action space of the joint effort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the action space\n",
    "def create_action_bins(num_action_bins):\n",
    "    '''\n",
    "    Using linspace of numpy implement the action bins for the pendulum, when given the number of the action bins as argument\n",
    "    '''\n",
    "    actionbins = np.linspace(-2.0, 2.0, num_action_bins)\n",
    "    \n",
    "    return actionbins\n",
    "\n",
    "# depending on the action, find the according actionbin \n",
    "# discretization of the continuous action space\n",
    "def find_actionbin(action, actionbins):\n",
    "    idx = (np.abs(actionbins - action)).argmin()\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward\n",
    "The reward is defined as\n",
    "> -(theta^2 + 0.1 x theta_dt^2 + 0.001 x action^2)\n",
    "\n",
    "What is the lowest expected cost? And what is the highest cost?\n",
    "\n",
    "-(pi^2 + 0.1 x 8^2 + 0.001 x 2^2) = -16.2736044\n",
    "\n",
    "-(0^2 + 0.1 x 0^2 + 0.001 x 0^2) = 0\n",
    "\n",
    "From this reward function, what is the goal of the agent?\n",
    "In essence, the goal is to remain at zero angle (vertical), with the least rotational velocity, and the least effort.\n",
    "\n",
    "For a hint have a look at the [wiki](https://github.com/openai/gym/wiki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(memory, gamma=0.9):\n",
    "    for state, action, reward, state_new in memory:\n",
    "        \n",
    "        # flatten state to make it compatible to our neural network\n",
    "        flat_state_new = np.reshape(state_new, [1, 3])\n",
    "        flat_state = np.reshape(state, [1, 3])\n",
    "\n",
    "        # determine estimated reward given state s' after action a, \n",
    "        # combination of observed and predicted exploited reward.\n",
    "        \n",
    "        target = reward + gamma * np.amax(model.predict(flat_state_new))\n",
    "        \n",
    "        # determine current expected agent rewards\n",
    "        targetfull = model.predict(flat_state)\n",
    "        \n",
    "        # update current expected rewards with the emulated prediced reward\n",
    "        targetfull[0][action] = target\n",
    "        \n",
    "        # Fit model based on emulation and prediction\n",
    "        model.fit(flat_state, targetfull, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "    \n",
    "    \n",
    "# Understanding the background concept:\n",
    "\n",
    "--\n",
    "\n",
    "   \n",
    "\n",
    "For training a NN model, we need the labels or outputs in case of supervised learning. In DQN, target is kind of used as label. Here target is generated as per the Bellman's principle. Using this target, we calculate the loss function mse and try to minise it while training.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Model\n",
    "\n",
    "As a reminder, this is our Q function.\n",
    "> Q(s, a) = r + gamma max_a'(Q(s, a'))\n",
    "\n",
    "The input of our neural network, our generalizable Q-matrix, will be the observation or the state of the pendulum. \n",
    "and the output will be the estimate of the reward taking the action a'. Gamma is the discount factor of the predicted reward in our next state. r is the reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first network we will implement a DQN with keras:\n",
    "\n",
    "- Layer with 128 ReLU units\n",
    "- Layer with 64 ReLU units\n",
    "- 3 inputs and one output per action bin with linear activation function\n",
    "- Adam optimizer with learning rate 0.0002, beta_1 0.9 and beta_2 0.999\n",
    "- Loss mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep-Q-Network in keras\n",
    "\n",
    "def build_model(num_output_nodes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_shape=(3,), activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_output_nodes, activation='linear'))\n",
    "    \n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(epsilon, gamma, training_iterations, sequence_iterations):\n",
    "    \n",
    "    # These are hyperparameters to play around with, after your first run.\n",
    "    epsilon_decay = 0.9999\n",
    "    epsilon_min = 0.02\n",
    "    steps_per_sequence = 250\n",
    "\n",
    "    for epoch in range(0, training_iterations // sequence_iterations):# for1\n",
    "        for sequence_id in range(0, sequence_iterations): # for2\n",
    "            state = env.reset()\n",
    "            memory = deque()\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            # Easy implementation of decaying exploration\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon = epsilon * epsilon_decay\n",
    "            \n",
    "            for i in range(0, steps_per_sequence): # for3\n",
    "                    \n",
    "                '''\n",
    "                Given epsilon implement a simple method for trading off exploration and exploitation\n",
    "            \n",
    "                Hint: For random values (use numpy) smaller than epsilon we want to explore\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                Hint: For random values larger than epsilon we want to exploit\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                if np.random.uniform() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else: # Exploitation = The greedy approach of choosing the action with high output value Q\n",
    "\n",
    "                    flat_state = np.reshape(state, [1, 3])\n",
    "                    action = np.amax(model.predict(flat_state)) \n",
    "                    \n",
    "                    \n",
    "                # determine action\n",
    "                actionbin = find_actionbin(action, actionbinslist)\n",
    "                action = actionbinslist[actionbin]\n",
    "                action = np.array([action])\n",
    "\n",
    "                # emulate the action in the simulation and observe the transition \n",
    "                # as well as the reward\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                state_new = observation\n",
    "\n",
    "                '''\n",
    "                save transitions into memory\n",
    "                Hint: The memory is used as an argument for the train_model function.\n",
    "                '''\n",
    "                \n",
    "                memory.append((state, actionbin, reward, state_new))\n",
    "                \n",
    "                \n",
    "                \n",
    "                state = state_new\n",
    "                \n",
    "            # train model on the samples from memory\n",
    "            train_model(memory, gamma)\n",
    "            \n",
    "            print(epoch , ' epoch', sequence_id, ' sequence. Average reward = ', total_reward / steps_per_sequence, '. epsilon = ', epsilon)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red;\">\n",
    "    \n",
    "# Understanding the background concept:\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "The inner most for loop (for3) runs 250 times and collects 250 samples of (s,a,r,s') and stores in the memory. This 250 samples are used to train the model once. Similarily we do the training for 25 times (sequence_iterations == 25) in for loop(for2). \n",
    "\n",
    "For every epoch, we train the model 25 times with 250 samples each time. \n",
    "We use here 40 epochs for training the model. (1000/25 in for loop (for1)).\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for running the policy of our DQN after loading or training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(rounds):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "\n",
    "    for _ in range(0, rounds):\n",
    "        # Rendering for visualization\n",
    "        env.render()\n",
    "\n",
    "        flat_state = np.reshape(state, [1, 3])\n",
    "        actionbin = np.argmax(model.predict(flat_state))\n",
    "\n",
    "        action = actionbinslist[actionbin]\n",
    "        action = np.array([action])\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        totalreward += reward\n",
    "\n",
    "        state_new = observation\n",
    "        state = state_new\n",
    "        \n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  epoch 0  sequence. Average reward =  -5.265124535395204 . epsilon =  0.9999\n",
      "0  epoch 1  sequence. Average reward =  -6.084306390158031 . epsilon =  0.9998000100000001\n",
      "0  epoch 2  sequence. Average reward =  -8.067130315879194 . epsilon =  0.9997000299990001\n",
      "0  epoch 3  sequence. Average reward =  -8.475199613461633 . epsilon =  0.9996000599960002\n",
      "0  epoch 4  sequence. Average reward =  -6.184959141074345 . epsilon =  0.9995000999900007\n",
      "0  epoch 5  sequence. Average reward =  -6.023470164568668 . epsilon =  0.9994001499800017\n",
      "0  epoch 6  sequence. Average reward =  -6.866023373383015 . epsilon =  0.9993002099650037\n",
      "0  epoch 7  sequence. Average reward =  -5.774279801564324 . epsilon =  0.9992002799440072\n",
      "0  epoch 8  sequence. Average reward =  -8.076870277917344 . epsilon =  0.9991003599160128\n",
      "0  epoch 9  sequence. Average reward =  -9.173715904890374 . epsilon =  0.9990004498800211\n",
      "0  epoch 10  sequence. Average reward =  -4.85829289998192 . epsilon =  0.9989005498350332\n",
      "0  epoch 11  sequence. Average reward =  -4.568279489127745 . epsilon =  0.9988006597800497\n",
      "0  epoch 12  sequence. Average reward =  -5.6830142455331405 . epsilon =  0.9987007797140718\n",
      "0  epoch 13  sequence. Average reward =  -5.774896078037394 . epsilon =  0.9986009096361004\n",
      "0  epoch 14  sequence. Average reward =  -6.16587626762547 . epsilon =  0.9985010495451367\n",
      "0  epoch 15  sequence. Average reward =  -7.304528978788412 . epsilon =  0.9984011994401822\n",
      "0  epoch 16  sequence. Average reward =  -6.436208865072617 . epsilon =  0.9983013593202382\n",
      "0  epoch 17  sequence. Average reward =  -4.41959827183356 . epsilon =  0.9982015291843062\n",
      "0  epoch 18  sequence. Average reward =  -4.823796416397109 . epsilon =  0.9981017090313877\n",
      "0  epoch 19  sequence. Average reward =  -6.795126466308097 . epsilon =  0.9980018988604846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-148118128c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_action_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mrun_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m '''\n",
      "\u001b[0;32m<ipython-input-5-6e4744e4c370>\u001b[0m in \u001b[0;36mrun_episodes\u001b[0;34m(epsilon, gamma, training_iterations, sequence_iterations)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# train model on the samples from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m' epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' sequence. Average reward = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_per_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'. epsilon = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f7438b46a0f8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(memory, gamma)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Fit model based on emulation and prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetfull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    385\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     dataset = dataset.map(\n\u001b[0m\u001b[1;32m    388\u001b[0m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1805\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m       return ParallelMapDataset(\n\u001b[0m\u001b[1;32m   1808\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4263\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m         **self._flat_structure)\n\u001b[0;32m-> 4265\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4267\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, variant_tensor)\u001b[0m\n\u001b[1;32m   3110\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3111\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3112\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnaryDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3114\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, variant_tensor)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mweak_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     self._variant_tracker = self._track_trackable(\n\u001b[0m\u001b[1;32m    203\u001b[0m         _VariantTracker(\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_track_trackable\u001b[0;34m(self, trackable, name, overwrite)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcurrent_object\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_unconditional_checkpoint_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_reference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_deferred_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_unconditional_dependency_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_handle_deferred_dependencies\u001b[0;34m(self, name, trackable)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;31m# Pass on any name-based restores queued in this object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m     for name_based_restore in sorted(\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_name_based_restores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_uid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "# These are hyperparameters to play around with\n",
    "\n",
    "# iterations\n",
    "training_iterations = 1000\n",
    "sequence_iterations = 25\n",
    "\n",
    "# epsilon (setting exploitation vs exploration)\n",
    "epsilon = 1\n",
    "\n",
    "# gamma (importance of predicted estimated reward)\n",
    "gamma = 0.9\n",
    "\n",
    "# Discretization settings for the action space\n",
    "num_action_bins = 20\n",
    "actionbinslist = create_action_bins(num_action_bins)\n",
    "\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = build_model(num_action_bins)\n",
    "\n",
    "run_episodes(epsilon, gamma, training_iterations, sequence_iterations)\n",
    "\n",
    "'''\n",
    "training takes super long, this is not efficient at all, how can we bypass this?\n",
    "Hint: See cells in run pretrained model.\n",
    "\n",
    "'''\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "    \n",
    "By loading the weights of a pretrained model.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model weights\n",
    "\n",
    "print('saving model')\n",
    "model.save('pendulum_model_juno_' + str(training_iterations) + '.h5')\n",
    "print('model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on 10 test runs with 100 steps each\n",
    "trarray = []\n",
    "rounds = 100\n",
    "for i in range(10):\n",
    "    trarray.append(play_game(rounds))\n",
    "    print(i, ' sequence. Average test reward = ', np.average(trarray)/rounds, 'Average test reward = ', trarray[-1]/rounds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pretrained model\n",
    "\n",
    "In case you already trained a model or want to load the pretrained model for sanity checking use the following script (make sure you executed the necessary cells starting with the imports).\n",
    "\n",
    "- How does the performance change with the amount of trained iterations?\n",
    "- How can we measure performance to begin with?\n",
    "- Is it sufficient to start the play_game function a single time? \n",
    "- How can we make sure, that the evaluation is meaningful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "actionbinslist = create_action_bins(20)\n",
    "\n",
    "# 'pendulum_model_[iterationstrained].h5' \n",
    "# iterationstrained: 100, 1000, 10000\n",
    "model = load_model('pendulum_model_1000.h5')\n",
    "\n",
    "'''\n",
    "Is the next line meaningful for evaluation, if not, what can we do?\n",
    "'''\n",
    "#lay_game(rounds=250)\n",
    "#play_game(rounds=2500)\n",
    "\n",
    "for i in range(3):\n",
    "    play_game(rounds=250)\n",
    "    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for - How does the performance change with the amount of trained iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "\n",
    "With less number of iterations trained, the model fails to understand that the vertical position (theta =0) is the ideal position or goal. \n",
    "\n",
    "But as we increase the training iterations, the model is able to stop after reaching the theta =0 position.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for - How can we measure performance to begin with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "    \n",
    "    \n",
    "To begin with, we start playing the game also specifying the maximum number of rounds. By restarting the game, we go to different initial positions and we can measure the number of times if we have reached the goal.\n",
    "    \n",
    "Example:\n",
    "    \n",
    "    \n",
    "    for i in range(50):\n",
    "    \n",
    "        play_game(rounds=250)\n",
    "    \n",
    "In this example, we start and play the game 50 times. This means 50 different random initial positions. Now the efficiency of our DQN could be calculated as  'x/50', where x = number times the game has reached the goal (Vertical position).\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for - Is it sufficient to start the play_game function a single time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "\n",
    "\n",
    "# Case1 - play_game(rounds=250) or play_game(rounds=2500):\n",
    "\n",
    "For Evaluation just writing play_game(rounds=250) or play_game(rounds=2500) is not appropiate.\n",
    "#### Reason:\n",
    "If the game reaches the goal or ends or gets into a loop with lessrounds (say 50), then this means optimal state is found and there is no need to change the state further and so the state remains the same for the rest of the rounds(200 rounds). \n",
    "\n",
    " \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for - How can we make sure, that the evaluation is meaningful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "\n",
    "# Case2 - multiple calls for play_game(rounds = 250):\n",
    "This case might be appropriate.\n",
    "    \n",
    "#### Reason: \n",
    "The play game is called multiple times and if we make the selected initial action random, then we can evaluate the model for different initial positions with this for loop (multiple calls).\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Implement a skip frame approach\n",
    "- Experiment with the discretization of the action bins (e.g. advantages and disadvantages of triadisation)\n",
    "- Experiment with exploration vs exploitation\n",
    "\n",
    "Send extended ipynb file to mark.schutera@kit.edu for the chance to get bonus points for the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "\n",
    "# Skip frame approach:\n",
    "\n",
    "The below is an implementation of skip_frame factor/parameter = 2. \n",
    "\n",
    "This means one in every set of two frames/observation from environment are used to form a new state and are feeded to the NN.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">\n",
    "\n",
    "# Exploitation and Exploration:\n",
    "Always choose the action that has highest Q value from the model. \n",
    "\n",
    "But initially exploration is good.\n",
    "\n",
    "### A faster epsilon_decay factor could be chosen and the we run episodes with epsilon = 1 => Exploration first. Then we decay the epsilon value at a higher rate (0.8) so as to go to Exploitation fast.\n",
    "    \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(epsilon, gamma, training_iterations, sequence_iterations):\n",
    "    \n",
    "    # These are hyperparameters to play around with, after your first run.\n",
    "    epsilon_decay = 0.8 \n",
    "    epsilon_min = 0.02\n",
    "    steps_per_sequence = 250\n",
    "\n",
    "    for epoch in range(0, training_iterations // sequence_iterations):# for1\n",
    "        for sequence_id in range(0, sequence_iterations): # for2\n",
    "            state = env.reset()\n",
    "            memory = deque()\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            # Easy implementation of decaying exploration\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon = epsilon * epsilon_decay\n",
    "            \n",
    "            for i in range(0, steps_per_sequence): # for3\n",
    "                \n",
    "                if i / 2 == 0: # --> Skip the even frames\n",
    "                    continue\n",
    "                \n",
    "                #if np.random.uniform() < epsilon:\n",
    "                #    action = env.action_space.sample()\n",
    "                #else: # Exploitation = The greedy approach of choosing the action with high output value Q\n",
    "\n",
    "                flat_state = np.reshape(state, [1, 3])\n",
    "                action = np.amax(model.predict(flat_state)) \n",
    "                    \n",
    "                \n",
    "                \n",
    "                # determine action\n",
    "                actionbin = find_actionbin(action, actionbinslist)\n",
    "                action = actionbinslist[actionbin]\n",
    "                action = np.array([action])\n",
    "\n",
    "                # emulate the action in the simulation and observe the transition \n",
    "                # as well as the reward\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                state_new = observation\n",
    "                \n",
    "                memory.append((state, actionbin, reward, state_new))\n",
    "                \n",
    "                state = state_new\n",
    "                \n",
    "            # train model on the samples from memory\n",
    "            train_model(memory, gamma)\n",
    "            \n",
    "            print(epoch , ' epoch', sequence_id, ' sequence. Average reward = ', total_reward / steps_per_sequence, '. epsilon = ', epsilon)\n",
    "\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
